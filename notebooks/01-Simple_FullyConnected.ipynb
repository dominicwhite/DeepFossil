{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Simple PyTorch",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dominicwhite/DeepFossil/blob/master/notebooks/01-Simple_FullyConnected.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNGix1yuQ8Zl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "28137144-77f1-4c04-a133-8fb2816021a6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hLUtn6gQl9G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "transform = transforms.Compose([\n",
        "    # you can add other transformations in this list\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "dataset = datasets.ImageFolder('/content/gdrive/My Drive/Colab Notebooks/data/CT/simulated_volumes/128', transform=transform)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kt9QlWLnRId-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "621389c6-a6e0-44b5-c051-1bcd4d6fab75"
      },
      "source": [
        "dataset.classes"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1', '2', '3', '4', '5']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1Gb2JX-RTaO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3e86IAMT2aq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "97d28c37-ac3f-4d6a-87dc-28dbb38687b5"
      },
      "source": [
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if not train_on_gpu:\n",
        "    print('CUDA is not available.  Training on CPU ...')\n",
        "else:\n",
        "    print('CUDA is available!  Training on GPU ...')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA is available!  Training on GPU ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcQD6e1mRU1N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "97de6a9d-b919-43eb-f23f-c3646dbb1fe6"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# define the CNN architecture\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # convolutional layer (sees 32x32x3 image tensor)\n",
        "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
        "        # convolutional layer (sees 16x16x16 tensor)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
        "        # convolutional layer (sees 8x8x32 tensor)\n",
        "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        # max pooling layer\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        # linear layer (64 * 4 * 4 -> 500)\n",
        "        self.fc1 = nn.Linear(64 * 16 * 16, 500)\n",
        "        # linear layer (500 -> 10)\n",
        "        self.fc2 = nn.Linear(500, 5)\n",
        "        # dropout layer (p=0.25)\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # add sequence of convolutional and max pooling layers\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        # flatten image input\n",
        "        print(\"Preflattened\", x.shape)\n",
        "        x = x.view(-1, 64 * 16 * 16)\n",
        "        print(\"Flattened\", x.shape)\n",
        "        # add dropout layer\n",
        "        x = self.dropout(x)\n",
        "        # add 1st hidden layer, with relu activation function\n",
        "        x = F.relu(self.fc1(x))\n",
        "        # add dropout layer\n",
        "        x = self.dropout(x)\n",
        "        # add 2nd hidden layer, with relu activation function\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# create a complete CNN\n",
        "model = Net()\n",
        "print(model)\n",
        "\n",
        "# move tensors to GPU if CUDA is available\n",
        "if train_on_gpu:\n",
        "    model.cuda()\n",
        "\n",
        "from torch import nn, optim\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
        "\n",
        "epochs = 5\n",
        "\n",
        "for e in range(epochs):\n",
        "    print(\"Starting epoch\", e)\n",
        "#     running_loss = 0\n",
        "    for images, labels in dataloader:\n",
        "        print(images.shape)\n",
        "        images, labels = images.to('cuda', dtype=torch.float), labels.to('cuda', dtype=torch.float)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(images)\n",
        "        print(output)\n",
        "        break\n",
        "#         loss = criterion(log_ps, labels)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=16384, out_features=500, bias=True)\n",
            "  (fc2): Linear(in_features=500, out_features=5, bias=True)\n",
            "  (dropout): Dropout(p=0.25)\n",
            ")\n",
            "Starting epoch 0\n",
            "torch.Size([32, 3, 128, 128])\n",
            "Preflattened torch.Size([32, 64, 16, 16])\n",
            "Flattened torch.Size([32, 16384])\n",
            "tensor([[-0.0198,  0.0069,  0.0014, -0.0042,  0.0225],\n",
            "        [-0.0270,  0.0190,  0.0019,  0.0068,  0.0221],\n",
            "        [-0.0212,  0.0158,  0.0081,  0.0181,  0.0157],\n",
            "        [-0.0215,  0.0158, -0.0004,  0.0096,  0.0246],\n",
            "        [-0.0342,  0.0048,  0.0004,  0.0021,  0.0109],\n",
            "        [-0.0252,  0.0189, -0.0027,  0.0027,  0.0161],\n",
            "        [-0.0292,  0.0236,  0.0082, -0.0004,  0.0162],\n",
            "        [-0.0197, -0.0008, -0.0094,  0.0246,  0.0238],\n",
            "        [-0.0302,  0.0135, -0.0022, -0.0019,  0.0188],\n",
            "        [-0.0288,  0.0184, -0.0009,  0.0009,  0.0192],\n",
            "        [-0.0375,  0.0164, -0.0053,  0.0022,  0.0002],\n",
            "        [-0.0310,  0.0142, -0.0200, -0.0154,  0.0121],\n",
            "        [-0.0215,  0.0102,  0.0006,  0.0128,  0.0171],\n",
            "        [-0.0397,  0.0245, -0.0072,  0.0086,  0.0161],\n",
            "        [-0.0425,  0.0236,  0.0053,  0.0033,  0.0201],\n",
            "        [-0.0203,  0.0070,  0.0043,  0.0173,  0.0161],\n",
            "        [-0.0207,  0.0122,  0.0093,  0.0050,  0.0167],\n",
            "        [-0.0277, -0.0057,  0.0036,  0.0013,  0.0185],\n",
            "        [-0.0282,  0.0123,  0.0047,  0.0068,  0.0164],\n",
            "        [-0.0277,  0.0283,  0.0021,  0.0052,  0.0204],\n",
            "        [-0.0400,  0.0204,  0.0139, -0.0039,  0.0040],\n",
            "        [-0.0197,  0.0240,  0.0014, -0.0052,  0.0155],\n",
            "        [-0.0272,  0.0201,  0.0064, -0.0010,  0.0200],\n",
            "        [-0.0249,  0.0169,  0.0065,  0.0036,  0.0157],\n",
            "        [-0.0308,  0.0230,  0.0061,  0.0044,  0.0186],\n",
            "        [-0.0282,  0.0279,  0.0064,  0.0050,  0.0063],\n",
            "        [-0.0225,  0.0229,  0.0080,  0.0037,  0.0212],\n",
            "        [-0.0255,  0.0164,  0.0092,  0.0051,  0.0207],\n",
            "        [-0.0323,  0.0245,  0.0103,  0.0128,  0.0122],\n",
            "        [-0.0209,  0.0099,  0.0250,  0.0097,  0.0240],\n",
            "        [-0.0278,  0.0168, -0.0013, -0.0005,  0.0232],\n",
            "        [-0.0402,  0.0285, -0.0085,  0.0226,  0.0213]], device='cuda:0',\n",
            "       grad_fn=<AddmmBackward>)\n",
            "Starting epoch 1\n",
            "torch.Size([32, 3, 128, 128])\n",
            "Preflattened torch.Size([32, 64, 16, 16])\n",
            "Flattened torch.Size([32, 16384])\n",
            "tensor([[-3.4301e-02,  1.7299e-02,  7.4537e-03,  7.4340e-03,  1.8385e-02],\n",
            "        [-3.2252e-02,  1.7952e-02, -3.4015e-03, -2.7563e-03,  1.4509e-02],\n",
            "        [-3.8084e-02,  1.3691e-02,  1.0309e-02, -1.3170e-02,  3.1440e-02],\n",
            "        [-3.3833e-02,  2.2026e-02, -7.8050e-05,  3.7865e-03,  1.2547e-02],\n",
            "        [-2.0311e-02, -4.4586e-03,  1.2338e-02,  3.5753e-03,  2.5862e-02],\n",
            "        [-3.2665e-02,  3.3956e-02, -4.3725e-03, -3.5656e-03,  9.8551e-03],\n",
            "        [-2.8419e-02,  2.2167e-02,  5.1479e-03,  2.8198e-03,  1.3482e-02],\n",
            "        [-3.0045e-02,  1.5509e-02, -3.4693e-03, -1.5712e-03,  1.9586e-02],\n",
            "        [-2.5989e-02,  9.2481e-03,  4.0405e-03,  1.8035e-02,  1.8665e-02],\n",
            "        [-3.1586e-02,  1.9078e-02, -1.0699e-02,  2.3736e-03,  2.1254e-02],\n",
            "        [-2.0241e-02,  2.1187e-02,  8.4276e-04,  2.0007e-03,  2.2821e-02],\n",
            "        [-2.7143e-02,  2.4870e-02, -7.0207e-03,  5.7730e-03,  1.9367e-02],\n",
            "        [-3.2345e-02,  1.9865e-02,  8.8072e-03, -3.9328e-03,  2.6214e-02],\n",
            "        [-2.1662e-02,  1.6452e-02,  4.9137e-03,  6.9888e-03,  1.6897e-02],\n",
            "        [-3.7887e-02,  6.4742e-03,  1.4754e-02, -3.7217e-03,  1.9243e-02],\n",
            "        [-1.4084e-02,  3.1807e-02,  1.2861e-03,  8.6621e-03,  3.8377e-03],\n",
            "        [-4.1738e-02,  2.6172e-02,  6.4334e-03,  2.9478e-03,  1.1052e-02],\n",
            "        [-2.9924e-02,  2.1214e-02,  8.7734e-03,  1.1556e-02,  1.1719e-02],\n",
            "        [-3.1383e-02,  2.0584e-02, -4.9023e-04,  1.1624e-02,  2.2692e-02],\n",
            "        [-3.2995e-02,  1.2550e-02,  1.7274e-02, -5.4401e-03,  1.5804e-02],\n",
            "        [-2.5063e-02,  1.6122e-02,  8.1756e-03, -2.4539e-04,  5.8636e-03],\n",
            "        [-2.3698e-02,  1.4237e-02,  1.1663e-02, -4.2200e-03,  5.8768e-03],\n",
            "        [-4.2164e-02,  1.9007e-02,  4.1093e-03,  3.7167e-03,  1.7694e-02],\n",
            "        [-2.6546e-02,  2.3154e-02, -2.4448e-03,  1.3668e-02,  3.1600e-02],\n",
            "        [-2.1021e-02,  2.4818e-02, -5.2386e-03,  1.9674e-02,  1.5017e-02],\n",
            "        [-2.4721e-02,  1.8446e-02,  5.1109e-03,  2.1424e-03,  1.8309e-02],\n",
            "        [-2.9681e-02,  1.6140e-02,  2.3365e-03,  1.2408e-02,  1.7883e-02],\n",
            "        [-3.0084e-02,  2.7841e-02,  4.1440e-03, -9.5465e-03,  1.2373e-02],\n",
            "        [-2.4050e-02,  7.5105e-03,  1.0269e-02,  8.2366e-03,  2.0809e-02],\n",
            "        [-2.2739e-02,  1.5685e-02, -2.6227e-03,  7.1219e-03,  1.3026e-02],\n",
            "        [-1.8406e-02,  9.7993e-03,  1.0301e-02,  5.8219e-04,  2.2611e-02],\n",
            "        [-2.4277e-02,  3.5069e-02,  2.5216e-03,  3.5480e-03,  1.3424e-02]],\n",
            "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "Starting epoch 2\n",
            "torch.Size([32, 3, 128, 128])\n",
            "Preflattened torch.Size([32, 64, 16, 16])\n",
            "Flattened torch.Size([32, 16384])\n",
            "tensor([[-2.6664e-02,  3.1403e-02,  1.1824e-02,  6.5987e-03,  1.2868e-02],\n",
            "        [-2.4530e-02,  2.2975e-02,  1.4855e-03, -1.6817e-03,  2.2341e-02],\n",
            "        [-3.8435e-02,  2.6605e-02,  2.1862e-03, -2.4332e-03,  2.0259e-02],\n",
            "        [-2.0609e-02,  2.7424e-02, -1.0308e-03, -4.0542e-03,  1.2895e-02],\n",
            "        [-1.4916e-02,  1.9905e-03,  1.0303e-02,  7.5948e-03,  2.7919e-02],\n",
            "        [-3.3763e-02,  2.1677e-02,  8.3072e-03, -5.9366e-03,  1.8511e-02],\n",
            "        [-1.9600e-02,  1.4078e-02,  5.8094e-03,  6.5901e-03,  2.5762e-02],\n",
            "        [-3.4727e-02,  1.5108e-02, -5.9821e-05,  5.5632e-03,  1.7404e-02],\n",
            "        [-2.1941e-02,  2.1397e-02,  9.5372e-03,  7.8157e-03,  1.9557e-02],\n",
            "        [-3.7281e-02,  2.1762e-02,  4.7266e-03, -1.1664e-02,  1.4942e-02],\n",
            "        [-3.5220e-02,  2.0592e-02,  1.6946e-03,  2.1487e-03,  2.5837e-02],\n",
            "        [-2.7742e-02,  1.2471e-02,  1.9159e-03,  9.4339e-04,  1.4214e-02],\n",
            "        [-2.6411e-02,  2.0990e-02,  4.2795e-03, -4.3854e-03,  1.5110e-02],\n",
            "        [-3.1331e-02,  2.1616e-02, -5.6154e-03,  1.7782e-03,  1.2580e-02],\n",
            "        [-2.1203e-02,  1.8267e-02, -1.1424e-03, -1.1112e-03,  1.2289e-02],\n",
            "        [-3.5605e-02,  2.4411e-02, -2.5853e-03, -2.0575e-03,  1.1287e-02],\n",
            "        [-2.8074e-02,  3.8325e-03, -4.9807e-03,  1.4445e-02,  8.1210e-03],\n",
            "        [-2.6778e-02,  2.0754e-02, -4.9397e-03, -1.0760e-02,  2.6090e-02],\n",
            "        [-2.1833e-02,  1.7856e-02, -1.7769e-03,  4.7576e-03,  1.7628e-02],\n",
            "        [-1.6802e-02,  2.2473e-02,  3.8346e-03,  1.7661e-03,  1.7185e-02],\n",
            "        [-2.9693e-02,  2.6614e-02,  1.0495e-02, -4.6918e-03,  1.3736e-02],\n",
            "        [-2.3271e-02,  2.0738e-02, -2.3489e-03,  2.6790e-03,  2.0211e-02],\n",
            "        [-3.0588e-02,  2.2610e-02,  3.9404e-03,  1.0689e-02,  1.7123e-02],\n",
            "        [-3.0883e-02,  1.4825e-02,  1.0938e-02, -6.5926e-04,  2.4451e-02],\n",
            "        [-2.0980e-02,  1.9386e-02, -5.8063e-03,  1.0856e-02,  3.6445e-03],\n",
            "        [-2.5434e-02,  2.2641e-02, -4.2552e-03,  1.5919e-02,  6.5994e-04],\n",
            "        [-2.6685e-02,  2.8452e-02,  5.3308e-03,  3.3788e-03,  2.1851e-02],\n",
            "        [-3.5021e-02,  2.5870e-02,  1.2669e-02,  1.1528e-02,  2.3908e-02],\n",
            "        [-4.0278e-02,  2.5415e-02,  5.8426e-04,  9.0408e-04,  2.6465e-02],\n",
            "        [-2.6878e-02,  1.3122e-02,  8.0872e-04, -6.1017e-03,  7.6177e-03],\n",
            "        [-2.5530e-02,  2.7791e-02,  1.7389e-03, -2.1955e-05,  2.7260e-02],\n",
            "        [-3.0968e-02,  7.2392e-03, -4.8246e-03, -3.6593e-03,  1.5881e-02]],\n",
            "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "Starting epoch 3\n",
            "torch.Size([32, 3, 128, 128])\n",
            "Preflattened torch.Size([32, 64, 16, 16])\n",
            "Flattened torch.Size([32, 16384])\n",
            "tensor([[-0.0185,  0.0077, -0.0075,  0.0061,  0.0034],\n",
            "        [-0.0297,  0.0216,  0.0055,  0.0087,  0.0136],\n",
            "        [-0.0273,  0.0113,  0.0160, -0.0096,  0.0164],\n",
            "        [-0.0248,  0.0178,  0.0032,  0.0076,  0.0205],\n",
            "        [-0.0300,  0.0122,  0.0033,  0.0104,  0.0294],\n",
            "        [-0.0378,  0.0046,  0.0040,  0.0075,  0.0075],\n",
            "        [-0.0486,  0.0258, -0.0002,  0.0029,  0.0160],\n",
            "        [-0.0324,  0.0140,  0.0177,  0.0089,  0.0253],\n",
            "        [-0.0345,  0.0144,  0.0023,  0.0074,  0.0172],\n",
            "        [-0.0347,  0.0278,  0.0132, -0.0004,  0.0200],\n",
            "        [-0.0188,  0.0177,  0.0123,  0.0024,  0.0117],\n",
            "        [-0.0089,  0.0247, -0.0016, -0.0087,  0.0156],\n",
            "        [-0.0261,  0.0209,  0.0052,  0.0114,  0.0198],\n",
            "        [-0.0221,  0.0136, -0.0066,  0.0028,  0.0199],\n",
            "        [-0.0259,  0.0261,  0.0112,  0.0067,  0.0135],\n",
            "        [-0.0247,  0.0171,  0.0060,  0.0073,  0.0297],\n",
            "        [-0.0271,  0.0177, -0.0158,  0.0017,  0.0286],\n",
            "        [-0.0239,  0.0256,  0.0051, -0.0020,  0.0132],\n",
            "        [-0.0246,  0.0298, -0.0057,  0.0080,  0.0214],\n",
            "        [-0.0306,  0.0200,  0.0028, -0.0002,  0.0221],\n",
            "        [-0.0326,  0.0155,  0.0093,  0.0120,  0.0229],\n",
            "        [-0.0322,  0.0215, -0.0008,  0.0035,  0.0137],\n",
            "        [-0.0232,  0.0246,  0.0058,  0.0065,  0.0319],\n",
            "        [-0.0147,  0.0166, -0.0067, -0.0017,  0.0105],\n",
            "        [-0.0126,  0.0075,  0.0141,  0.0050,  0.0124],\n",
            "        [-0.0304,  0.0260,  0.0131, -0.0026,  0.0247],\n",
            "        [-0.0277,  0.0265,  0.0034, -0.0068,  0.0225],\n",
            "        [-0.0061, -0.0019, -0.0005, -0.0035,  0.0266],\n",
            "        [-0.0291,  0.0264,  0.0067, -0.0005,  0.0184],\n",
            "        [-0.0308,  0.0324, -0.0007,  0.0017,  0.0232],\n",
            "        [-0.0286,  0.0227,  0.0081,  0.0087,  0.0298],\n",
            "        [-0.0372,  0.0223, -0.0010,  0.0058,  0.0308]], device='cuda:0',\n",
            "       grad_fn=<AddmmBackward>)\n",
            "Starting epoch 4\n",
            "torch.Size([32, 3, 128, 128])\n",
            "Preflattened torch.Size([32, 64, 16, 16])\n",
            "Flattened torch.Size([32, 16384])\n",
            "tensor([[-2.6524e-02,  2.2965e-02,  2.6660e-03,  7.9686e-04,  7.3764e-03],\n",
            "        [-3.2566e-02,  1.7017e-02,  1.0533e-02,  2.8609e-03,  1.6927e-02],\n",
            "        [-3.3844e-02,  1.4696e-02,  7.8565e-03, -2.1625e-03,  2.0239e-02],\n",
            "        [-4.1852e-02,  1.6272e-02,  5.1366e-03,  5.7916e-03,  2.2097e-02],\n",
            "        [-3.8311e-02,  1.8994e-02, -8.6446e-04,  3.0280e-03,  2.3019e-02],\n",
            "        [-2.3875e-02,  1.1384e-02,  1.6861e-03, -1.1504e-02,  1.6938e-03],\n",
            "        [-2.6521e-02,  1.7813e-02, -1.0970e-03, -2.3085e-03,  2.1377e-02],\n",
            "        [-3.2204e-02,  2.1104e-02,  2.2948e-03,  5.0630e-03,  2.6167e-02],\n",
            "        [-3.3673e-02,  8.2256e-03,  6.1984e-03,  2.0296e-02,  2.6520e-02],\n",
            "        [-2.2476e-02,  4.2015e-03,  1.9614e-03, -2.2674e-02,  3.1460e-02],\n",
            "        [-3.7039e-02,  7.5933e-03,  1.8590e-03,  3.6440e-03,  2.5126e-02],\n",
            "        [-2.6709e-02,  1.2466e-02,  1.3944e-03,  2.0824e-03,  7.6823e-03],\n",
            "        [-1.8642e-02,  1.6983e-02,  7.1764e-03,  1.4772e-03,  1.6459e-02],\n",
            "        [-3.4399e-02,  2.7876e-02, -1.5629e-03,  3.0526e-03,  1.7408e-02],\n",
            "        [-1.4423e-02,  2.0407e-02,  2.9693e-03, -4.0279e-03,  9.5249e-03],\n",
            "        [-2.3964e-02,  1.7620e-02,  3.5983e-03, -5.6336e-03,  1.9112e-02],\n",
            "        [-2.6412e-02,  1.2838e-02,  1.5976e-02,  8.4379e-03,  2.5428e-02],\n",
            "        [-2.8565e-02,  2.2453e-02,  1.0281e-02,  1.0195e-02,  1.1732e-02],\n",
            "        [-2.5767e-02,  2.7393e-02, -2.1653e-03,  1.8158e-03,  1.7737e-02],\n",
            "        [-1.2964e-02, -1.7366e-03,  1.0576e-02, -3.0682e-03,  2.5167e-02],\n",
            "        [-3.4574e-02,  2.3775e-02,  5.1043e-03, -5.2936e-03,  7.2734e-03],\n",
            "        [-2.3067e-02,  3.8301e-04,  8.3460e-03,  3.2337e-03,  2.0349e-02],\n",
            "        [-3.9419e-02,  9.7992e-03,  6.0730e-03,  9.1209e-03,  8.5656e-03],\n",
            "        [-2.9812e-02,  2.2229e-02,  5.4146e-03,  4.0469e-03,  2.2437e-02],\n",
            "        [-3.2804e-02,  1.4955e-02,  5.9280e-03,  4.4453e-03,  2.5297e-02],\n",
            "        [-3.9225e-02,  1.4644e-02,  9.5410e-05,  4.4912e-03,  2.4379e-02],\n",
            "        [-2.8864e-02,  1.8694e-02, -2.3992e-03,  8.5947e-04,  8.6923e-03],\n",
            "        [-2.5119e-02,  2.5120e-02,  9.1988e-03, -9.1388e-03,  9.8858e-03],\n",
            "        [-2.3758e-02,  1.3644e-02,  4.7952e-04, -6.1436e-03,  2.0584e-02],\n",
            "        [-1.6684e-02,  2.9187e-02,  9.9275e-03, -1.0036e-04,  2.0105e-02],\n",
            "        [-2.9902e-02,  2.5975e-02,  4.3755e-03, -2.5378e-03,  2.3526e-02],\n",
            "        [-2.7612e-02,  7.4514e-03,  6.8605e-03,  2.2865e-03,  3.2352e-02]],\n",
            "       device='cuda:0', grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdjVUsd7T1Za",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "f0858ff4-75b9-48b5-f1f9-6fdb39905a49"
      },
      "source": [
        "images, labels = next(iter(dataloader))\n",
        "images = images.to('cuda')\n",
        "model(images)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0291, -0.0045, -0.0497, -0.0138,  0.0146],\n",
              "        [-0.0356, -0.0006, -0.0399, -0.0212,  0.0159],\n",
              "        [-0.0246,  0.0035, -0.0451, -0.0408,  0.0009],\n",
              "        ...,\n",
              "        [-0.0200,  0.0015, -0.0523, -0.0229,  0.0171],\n",
              "        [-0.0308, -0.0036, -0.0457, -0.0265,  0.0115],\n",
              "        [-0.0490, -0.0111, -0.0382, -0.0078,  0.0006]], device='cuda:0',\n",
              "       grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GETCywXzWWgu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "62c75337-8e46-42b4-a0c6-daba67aef734"
      },
      "source": [
        "images[0][0,:,:]"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_SDCUzpWroW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}